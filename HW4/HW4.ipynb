{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet18\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset class\n",
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.samples = [f for f in os.listdir(root_dir) if f.endswith('.png')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, self.samples[idx])\n",
    "        img = cv2.imread(img_name)\n",
    "        \n",
    "        if self.transform:\n",
    "                img = self.transform(img)\n",
    "        \n",
    "        if self.root_dir.split('_')[2]=='train':\n",
    "            \n",
    "            label_file = img_name.replace('.png', '.txt')\n",
    "            with open(label_file, 'r') as f:\n",
    "                label = int(f.readline()[0])\n",
    "\n",
    "            return img, label\n",
    "        else:\n",
    "            return img, self.samples[idx]\n",
    "\n",
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "\n",
    "def create_stratified_split(dataset, test_size=0.2):\n",
    "    labels = []\n",
    "    for _, label_path in enumerate(dataset.samples):\n",
    "        label_file = os.path.join(dataset.root_dir, label_path).replace('.png', '.txt')\n",
    "        with open(label_file, 'r') as f:\n",
    "            label = int(f.readline().strip()[0])\n",
    "        labels.append(label)\n",
    "\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=0)\n",
    "    train_idx, valid_idx = next(sss.split(np.zeros(len(labels)), labels))\n",
    "    return train_idx, valid_idx\n",
    "\n",
    "# Create dataset\n",
    "train_dataset = MNISTDataset('../HW2/HW2_MNIST_train', transform=transform)\n",
    "\n",
    "# If val_flag is true, create stratified split\n",
    "val_flag = True\n",
    "if val_flag:\n",
    "    train_idx, valid_idx = create_stratified_split(train_dataset)\n",
    "\n",
    "    # Create samplers\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "    # Create loaders\n",
    "    batch_size = 64\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "    valid_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=valid_sampler)\n",
    "else:\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set class counts: {6: 7579, 7: 8005, 5: 6950, 1: 8668, 2: 7625, 8: 7542, 9: 7619, 4: 7451, 0: 8628, 3: 7849}\n",
      "Validation set class counts: {4: 1863, 2: 1906, 3: 1963, 5: 1737, 6: 1895, 9: 1905, 8: 1885, 0: 2157, 1: 2167, 7: 2002}\n"
     ]
    }
   ],
   "source": [
    "def count_classes(dataset, indices):\n",
    "    class_counts = {}\n",
    "    for idx in indices:\n",
    "        img_name = os.path.join(dataset.root_dir, dataset.samples[idx])\n",
    "        label_file = img_name.replace('.png', '.txt')\n",
    "        with open(label_file, 'r') as f:\n",
    "            label = int(f.readline().strip()[0])\n",
    "        if label in class_counts:\n",
    "            class_counts[label] += 1\n",
    "        else:\n",
    "            class_counts[label] = 1\n",
    "    return class_counts\n",
    "\n",
    "# Count classes in the training set\n",
    "train_class_counts = count_classes(train_dataset, train_idx)\n",
    "print(\"Training set class counts:\", train_class_counts)\n",
    "\n",
    "# Count classes in the validation set\n",
    "valid_class_counts = count_classes(train_dataset, valid_idx)\n",
    "print(\"Validation set class counts:\", valid_class_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Start training SimpleRNN:\n",
      "Epoch [1/15], SimpleRNN Train Loss: 2.3194, Train Accuracy: 0.1039, Val Loss: 2.3099, Val Accuracy: 0.1112\n",
      "Epoch [2/15], SimpleRNN Train Loss: 2.3147, Train Accuracy: 0.1030, Val Loss: 2.3375, Val Accuracy: 0.1112\n",
      "Epoch [3/15], SimpleRNN Train Loss: 2.3190, Train Accuracy: 0.1031, Val Loss: 2.3236, Val Accuracy: 0.1107\n",
      "Epoch [4/15], SimpleRNN Train Loss: 2.3186, Train Accuracy: 0.1039, Val Loss: 2.3142, Val Accuracy: 0.1107\n",
      "Epoch [5/15], SimpleRNN Train Loss: 2.3208, Train Accuracy: 0.1034, Val Loss: 2.3067, Val Accuracy: 0.1112\n",
      "Epoch [6/15], SimpleRNN Train Loss: 2.3219, Train Accuracy: 0.1045, Val Loss: 2.3239, Val Accuracy: 0.1008\n",
      "Epoch [7/15], SimpleRNN Train Loss: 2.3226, Train Accuracy: 0.1032, Val Loss: 2.3096, Val Accuracy: 0.1008\n",
      "Epoch [8/15], SimpleRNN Train Loss: 2.3216, Train Accuracy: 0.1034, Val Loss: 2.3097, Val Accuracy: 0.1028\n",
      "Epoch [9/15], SimpleRNN Train Loss: 2.3227, Train Accuracy: 0.1011, Val Loss: 2.3087, Val Accuracy: 0.1112\n",
      "Epoch [10/15], SimpleRNN Train Loss: 2.3220, Train Accuracy: 0.1032, Val Loss: 2.3419, Val Accuracy: 0.1107\n",
      "Epoch [11/15], SimpleRNN Train Loss: 2.3222, Train Accuracy: 0.1044, Val Loss: 2.3103, Val Accuracy: 0.1091\n",
      "Epoch [12/15], SimpleRNN Train Loss: 2.3231, Train Accuracy: 0.1038, Val Loss: 2.3142, Val Accuracy: 0.1044\n",
      "Epoch [13/15], SimpleRNN Train Loss: 2.3193, Train Accuracy: 0.1047, Val Loss: 2.3107, Val Accuracy: 0.1146\n",
      "Epoch [14/15], SimpleRNN Train Loss: 2.3207, Train Accuracy: 0.1045, Val Loss: 2.3239, Val Accuracy: 0.0986\n",
      "Epoch [15/15], SimpleRNN Train Loss: 2.3213, Train Accuracy: 0.1043, Val Loss: 2.3231, Val Accuracy: 0.0992\n",
      "Start training LSTM:\n",
      "Epoch [1/15], LSTM Train Loss: 2.3012, Train Accuracy: 0.1111, Val Loss: 2.3008, Val Accuracy: 0.1107\n",
      "Epoch [2/15], LSTM Train Loss: 2.3009, Train Accuracy: 0.1110, Val Loss: 2.3007, Val Accuracy: 0.1107\n",
      "Epoch [3/15], LSTM Train Loss: 2.3008, Train Accuracy: 0.1104, Val Loss: 2.3007, Val Accuracy: 0.1107\n",
      "Epoch [4/15], LSTM Train Loss: 2.3008, Train Accuracy: 0.1107, Val Loss: 2.3006, Val Accuracy: 0.1107\n",
      "Epoch [5/15], LSTM Train Loss: 2.3007, Train Accuracy: 0.1101, Val Loss: 2.3010, Val Accuracy: 0.1107\n",
      "Epoch [6/15], LSTM Train Loss: 2.3008, Train Accuracy: 0.1092, Val Loss: 2.3006, Val Accuracy: 0.1112\n",
      "Epoch [7/15], LSTM Train Loss: 2.3007, Train Accuracy: 0.1118, Val Loss: 2.3006, Val Accuracy: 0.1107\n",
      "Epoch [8/15], LSTM Train Loss: 2.3007, Train Accuracy: 0.1109, Val Loss: 2.3006, Val Accuracy: 0.1112\n",
      "Epoch [9/15], LSTM Train Loss: 2.3007, Train Accuracy: 0.1104, Val Loss: 2.3006, Val Accuracy: 0.1107\n",
      "Epoch [10/15], LSTM Train Loss: 2.3007, Train Accuracy: 0.1102, Val Loss: 2.3007, Val Accuracy: 0.1107\n",
      "Epoch [11/15], LSTM Train Loss: 2.3007, Train Accuracy: 0.1109, Val Loss: 2.3006, Val Accuracy: 0.1107\n",
      "Epoch [12/15], LSTM Train Loss: 2.3007, Train Accuracy: 0.1107, Val Loss: 2.3005, Val Accuracy: 0.1112\n",
      "Epoch [13/15], LSTM Train Loss: 2.3006, Train Accuracy: 0.1101, Val Loss: 2.3011, Val Accuracy: 0.1112\n",
      "Epoch [14/15], LSTM Train Loss: 2.3007, Train Accuracy: 0.1097, Val Loss: 2.3006, Val Accuracy: 0.1112\n",
      "Epoch [15/15], LSTM Train Loss: 2.3007, Train Accuracy: 0.1098, Val Loss: 2.3007, Val Accuracy: 0.1112\n",
      "Start training GRU:\n",
      "Epoch [1/15], GRU Train Loss: 0.9236, Train Accuracy: 0.6658, Val Loss: 0.2909, Val Accuracy: 0.9063\n",
      "Epoch [2/15], GRU Train Loss: 0.2159, Train Accuracy: 0.9298, Val Loss: 0.1824, Val Accuracy: 0.9408\n",
      "Epoch [3/15], GRU Train Loss: 0.1402, Train Accuracy: 0.9542, Val Loss: 0.1636, Val Accuracy: 0.9492\n",
      "Epoch [4/15], GRU Train Loss: 0.1080, Train Accuracy: 0.9639, Val Loss: 0.1389, Val Accuracy: 0.9564\n",
      "Epoch [5/15], GRU Train Loss: 0.0810, Train Accuracy: 0.9727, Val Loss: 0.1335, Val Accuracy: 0.9603\n",
      "Epoch [6/15], GRU Train Loss: 0.0686, Train Accuracy: 0.9773, Val Loss: 0.1468, Val Accuracy: 0.9572\n",
      "Epoch [7/15], GRU Train Loss: 0.0589, Train Accuracy: 0.9799, Val Loss: 0.1421, Val Accuracy: 0.9605\n",
      "Epoch [8/15], GRU Train Loss: 0.0489, Train Accuracy: 0.9841, Val Loss: 0.1418, Val Accuracy: 0.9599\n",
      "Epoch [9/15], GRU Train Loss: 0.0464, Train Accuracy: 0.9848, Val Loss: 0.1530, Val Accuracy: 0.9593\n",
      "Epoch [10/15], GRU Train Loss: 0.0411, Train Accuracy: 0.9864, Val Loss: 0.1363, Val Accuracy: 0.9636\n",
      "Epoch [11/15], GRU Train Loss: 0.0376, Train Accuracy: 0.9876, Val Loss: 0.1616, Val Accuracy: 0.9573\n",
      "Epoch [12/15], GRU Train Loss: 0.0324, Train Accuracy: 0.9892, Val Loss: 0.1509, Val Accuracy: 0.9629\n",
      "Epoch [13/15], GRU Train Loss: 0.0331, Train Accuracy: 0.9888, Val Loss: 0.1422, Val Accuracy: 0.9632\n",
      "Epoch [14/15], GRU Train Loss: 0.0321, Train Accuracy: 0.9893, Val Loss: 0.1424, Val Accuracy: 0.9650\n",
      "Epoch [15/15], GRU Train Loss: 0.0261, Train Accuracy: 0.9911, Val Loss: 0.1522, Val Accuracy: 0.9640\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datetime import datetime\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# RNN models\n",
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, rnn_type):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        if rnn_type == 'SimpleRNN':\n",
    "            self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        elif rnn_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        elif rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, seq_len, input_size)\n",
    "        out, _ = self.rnn(x)\n",
    "        # take the last sequence to output\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# hyperparameters\n",
    "input_size = 256  # patch size (16x16)\n",
    "hidden_size = [64,64,256]  # RNN hidden layer\n",
    "num_layers = [1,2,4]  # RNN layers\n",
    "num_classes = 10  \n",
    "seq_length = 64  # 圖像轉換成序列的長度 (128x128 圖像分割成8x8塊，每塊16x16)\n",
    "\n",
    "# define model\n",
    "models = {\n",
    "    'SimpleRNN': RNNClassifier(input_size, hidden_size[0], num_layers[0], num_classes, 'SimpleRNN'),\n",
    "    'LSTM': RNNClassifier(input_size, hidden_size[1], num_layers[1], num_classes, 'LSTM'),\n",
    "    'GRU': RNNClassifier(input_size, hidden_size[2], num_layers[2], num_classes, 'GRU'),\n",
    "}\n",
    "\n",
    "# loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizers = {m: optim.Adam(model.parameters(), lr=0.001) for m, model in models.items()}\n",
    "\n",
    "# function: image to sequence\n",
    "def image_to_seq(image, block_size=16):\n",
    "    # segment image into patches \n",
    "    patches = image.unfold(2, block_size, block_size).unfold(3, block_size, block_size)\n",
    "    patches = patches.contiguous().view(image.size(0), -1, block_size*block_size)\n",
    "    return patches\n",
    "\n",
    "# training epoch\n",
    "num_epochs = 15\n",
    "for model_name, model in models.items():\n",
    "    print(f'Start training {model_name}:')\n",
    "    model = model.to(device)\n",
    "    optimizer = optimizers[model_name]\n",
    "    best_val_acc = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss, train_correct, train_total = 0.0, 0, 0\n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            images_seq = image_to_seq(images)\n",
    "            \n",
    "            outputs = model(images_seq)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        \n",
    "        train_acc = train_correct / train_total\n",
    "        \n",
    "        if val_flag:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for images, labels in valid_loader:\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "                    images_seq = image_to_seq(images)\n",
    "                    outputs = model(images_seq)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    val_loss += loss.item()\n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    val_total += labels.size(0)\n",
    "                    val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            val_acc = val_correct / val_total\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], {model_name} Train Loss: {train_loss / len(train_loader):.4f}, Train Accuracy: {train_acc:.4f}, Val Loss: {val_loss / len(valid_loader):.4f}, Val Accuracy: {val_acc:.4f}')\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                torch.save(model.state_dict(), f'best_{model_name}.pth')\n",
    "        else:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], {model_name} Train Loss: {train_loss / len(train_loader):.4f}, Train Accuracy: {train_acc:.4f}')\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    torch.save(model.state_dict(), f'{model_name}_{timestamp}.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters(SimpleRNN):  528906\n",
      "Number of trainable parameters(LSTM):  2107914\n",
      "Number of trainable parameters(GRU):  1581578\n"
     ]
    }
   ],
   "source": [
    "# Number of layers and parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "num_parameters = count_parameters(models['SimpleRNN'])\n",
    "print(\"Number of trainable parameters(SimpleRNN): \", num_parameters)\n",
    "\n",
    "num_parameters = count_parameters(models['LSTM'])\n",
    "print(\"Number of trainable parameters(LSTM): \", num_parameters)\n",
    "\n",
    "num_parameters = count_parameters(models['GRU'])\n",
    "print(\"Number of trainable parameters(GRU): \", num_parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = MNISTDataset('../HW2/HW2_MNIST_test', transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "model = RNNClassifier(input_size, hidden_size, num_layers, num_classes, 'GRU')\n",
    "\n",
    "# load model\n",
    "model_path = 'best_GRU.pth'  \n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "# Perform predictions on the test set\n",
    "model.eval()  \n",
    "predictions = []\n",
    "image_files = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, file_names = data\n",
    "        images = images.to(device)\n",
    "        images_seq = image_to_seq(images)\n",
    "        outputs = model(images_seq)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        predictions.extend(predicted.cpu().numpy())\n",
    "        image_files.extend(file_names)\n",
    "\n",
    "# Save to CSV\n",
    "predictions_df = pd.DataFrame({\n",
    "    'image': image_files,\n",
    "    'class': predictions\n",
    "})\n",
    "predictions_df.to_csv('HW4_prob1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 1, 2])\n",
      "Epoch [100/1000], Loss: 0.0000\n",
      "Epoch [200/1000], Loss: 0.0000\n",
      "Epoch [300/1000], Loss: 0.0000\n",
      "Epoch [400/1000], Loss: 0.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 56\u001b[0m\n\u001b[0;32m     52\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(batch_features)\n\u001b[0;32m     53\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, batch_features)\n\u001b[1;32m---> 56\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     57\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     58\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\optim\\optimizer.py:435\u001b[0m, in \u001b[0;36mOptimizer.zero_grad\u001b[1;34m(self, set_to_none)\u001b[0m\n\u001b[0;32m    431\u001b[0m     param_groups \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    432\u001b[0m         update_group(g, ng) \u001b[38;5;28;01mfor\u001b[39;00m g, ng \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(groups, saved_groups)]\n\u001b[0;32m    433\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__setstate__({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate\u001b[39m\u001b[38;5;124m'\u001b[39m: state, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparam_groups\u001b[39m\u001b[38;5;124m'\u001b[39m: param_groups})\n\u001b[1;32m--> 435\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mzero_grad\u001b[39m(\u001b[38;5;28mself\u001b[39m, set_to_none: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    436\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sets the gradients of all optimized :class:`torch.Tensor` s to zero.\u001b[39;00m\n\u001b[0;32m    437\u001b[0m \n\u001b[0;32m    438\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;124;03m            the step altogether).\u001b[39;00m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    450\u001b[0m     foreach \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforeach\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "data_path = './HW4_prob2.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# data\n",
    "X = torch.tensor(data.values, dtype=torch.float32)\n",
    "\n",
    "# use data as inputs and outputs\n",
    "dataset = TensorDataset(X, X)\n",
    "data_loader = DataLoader(dataset, batch_size=10, shuffle=True)\n",
    "\n",
    "# define model\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "input_size = X.size(1)\n",
    "hidden_size = 50  \n",
    "output_size = input_size  \n",
    "model = Generator(input_size, hidden_size, output_size)\n",
    "\n",
    "# loss function and optimizer\n",
    "criterion = nn.MSELoss()  \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "num_epochs = 1000  \n",
    "losses = []  \n",
    "for epoch in range(num_epochs):\n",
    "    for batch_features, _ in data_loader:\n",
    "        \n",
    "        batch_features = batch_features.to(torch.float32)\n",
    "        outputs = model(batch_features)\n",
    "        loss = criterion(outputs, batch_features)\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "with torch.no_grad():\n",
    "    new_data = model(X).numpy()\n",
    "\n",
    "plt.scatter(data['U1'], data['U2'], color='blue', alpha=0.5, label='Original Data')\n",
    "plt.scatter(new_data[:, 0], new_data[:, 1], color='red', alpha=0.5, label='Generated Data')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mGRUModel\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_size, hidden_size, num_layers, output_size):\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m(GRUModel, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        out, hidden = self.gru(x, hidden)\n",
    "        out = self.fc(out[:, -1, :])  \n",
    "        return out, hidden\n",
    "\n",
    "input_size = 2  \n",
    "hidden_size = 50  \n",
    "num_layers = 1  \n",
    "output_size = input_size  \n",
    "gru_model = GRUModel(input_size, hidden_size, num_layers, output_size).to(device)\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(gru_model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "X_gru = X.unsqueeze(1)  \n",
    "\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    hidden = None  \n",
    "    for batch_features, _ in data_loader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        \n",
    "        batch_features = batch_features.unsqueeze(1).to(device)\n",
    "        \n",
    "        \n",
    "        outputs, hidden = gru_model(batch_features, hidden)\n",
    "        # 我们不希望隐藏状态被梯度下降步骤影响\n",
    "        # 因此我们将其从历史中分离出来\n",
    "        hidden = hidden.detach()\n",
    "        \n",
    "        loss = criterion(outputs, batch_features[:, -1, :])\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # output every 100 epochs\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    # 由于我们的数据不是真正的时序数据，我们将整个数据作为一个批次进行处理\n",
    "    # 这样我们就可以生成与原数据相同数量的新数据点\n",
    "    X_gru = X_gru.to(device)\n",
    "    generated_data, _ = gru_model(X_gru, None)\n",
    "    generated_data = generated_data.cpu().numpy()\n",
    "\n",
    "# 绘制原始数据点和生成的数据点\n",
    "plt.scatter(data['U1'], data['U2'], color='blue', alpha=0.5, label='Original Data')\n",
    "plt.scatter(generated_data[:, 0], generated_data[:, 1], color='red', alpha=0.5, label='Generated Data')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
